{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d0MYAA8a7RG",
        "outputId": "3cc0af23-6e2a-41fc-ec6b-3ac47718d9b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Using cached ultralytics-8.3.229-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy<=2.3.4,>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib<=3.10.7,>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python<=4.12.0.88,>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow<=12.0.0,>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml<=6.0.3,>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests<=2.32.5,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy<=1.16.3,>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch<=2.9.1,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision<=0.24.1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil<=7.1.3,>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars<=1.35.2,>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop<=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10.7,>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.23.0->ultralytics) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib<=3.10.7,>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.9.1,>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.9.1,>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.229-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.229 ultralytics-thop-2.0.18\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install opencv-python\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class VideoObjectDetector:\n",
        "    def __init__(self, model_name='yolov8n.pt', conf_threshold=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the object detector\n",
        "\n",
        "        Args:\n",
        "            model_name: YOLO model to use\n",
        "            conf_threshold: Confidence threshold for detections\n",
        "        \"\"\"\n",
        "        self.model = YOLO(model_name)\n",
        "        self.conf_threshold = conf_threshold\n",
        "        self.class_names = self.model.names\n",
        "\n",
        "    def calculate_iou(self, box1, box2):\n",
        "        \"\"\"\n",
        "        Calculate Intersection over Union (IoU) between two bounding boxes\n",
        "\n",
        "        Args:\n",
        "            box1: [x1, y1, x2, y2] format\n",
        "            box2: [x1, y1, x2, y2] format\n",
        "\n",
        "        Returns:\n",
        "            iou: IoU value between 0 and 1\n",
        "        \"\"\"\n",
        "        # Calculate intersection area\n",
        "        x1_inter = max(box1[0], box2[0])\n",
        "        y1_inter = max(box1[1], box2[1])\n",
        "        x2_inter = min(box1[2], box2[2])\n",
        "        y2_inter = min(box1[3], box2[3])\n",
        "\n",
        "        # Calculate intersection area\n",
        "        inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
        "\n",
        "        # Calculate union area\n",
        "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "        union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if union_area == 0:\n",
        "            return 0\n",
        "\n",
        "        return inter_area / union_area\n",
        "\n",
        "    def process_video(self, input_video_path, output_video_path, target_class='person'):\n",
        "        \"\"\"\n",
        "        Process video and detect objects\n",
        "\n",
        "        Args:\n",
        "            input_video_path: Path to input video\n",
        "            output_video_path: Path to save output video\n",
        "            target_class: Class to detect (e.g., 'person', 'car', etc.)\n",
        "\n",
        "        Returns:\n",
        "            results: Dictionary containing detection results\n",
        "        \"\"\"\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "        # Get video properties\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Initialize video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "        # Storage for results\n",
        "        all_detections = []\n",
        "        frame_count = 0\n",
        "\n",
        "        print(f\"Processing video: {input_video_path}\")\n",
        "        print(f\"Total frames: {total_frames}\")\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Perform detection\n",
        "            results = self.model(frame, conf=self.conf_threshold, verbose=False)\n",
        "\n",
        "            # Process detections\n",
        "            frame_detections = []\n",
        "            for result in results:\n",
        "                boxes = result.boxes\n",
        "                if boxes is not None:\n",
        "                    for box in boxes:\n",
        "                        # Get box coordinates and class\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                        conf = box.conf[0].cpu().numpy()\n",
        "                        class_id = int(box.cls[0].cpu().numpy())\n",
        "                        class_name = self.class_names[class_id]\n",
        "\n",
        "                        # Filter by target class\n",
        "                        if class_name == target_class:\n",
        "                            frame_detections.append({\n",
        "                                'frame': frame_count,\n",
        "                                'class': class_name,\n",
        "                                'confidence': float(conf),\n",
        "                                'bbox': [float(x1), float(y1), float(x2), float(y2)]\n",
        "                            })\n",
        "\n",
        "                            # Draw bounding box\n",
        "                            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "                            label = f\"{class_name} {conf:.2f}\"\n",
        "                            cv2.putText(frame, label, (int(x1), int(y1)-10),\n",
        "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "            all_detections.extend(frame_detections)\n",
        "\n",
        "            # Write frame to output video\n",
        "            out.write(frame)\n",
        "\n",
        "            frame_count += 1\n",
        "            if frame_count % 30 == 0:\n",
        "                print(f\"Processed {frame_count}/{total_frames} frames\")\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        return {\n",
        "            'detections': all_detections,\n",
        "            'video_info': {\n",
        "                'fps': fps,\n",
        "                'width': width,\n",
        "                'height': height,\n",
        "                'total_frames': total_frames\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def evaluate_detections(self, detections, ground_truth, iou_threshold=0.5):\n",
        "        \"\"\"\n",
        "        Evaluate detection results against ground truth\n",
        "\n",
        "        Args:\n",
        "            detections: List of detected objects\n",
        "            ground_truth: Dictionary with ground truth data\n",
        "            iou_threshold: IoU threshold for considering a detection as correct\n",
        "\n",
        "        Returns:\n",
        "            evaluation_metrics: Dictionary with evaluation results\n",
        "        \"\"\"\n",
        "        true_positives = 0\n",
        "        false_positives = 0\n",
        "        false_negatives = 0\n",
        "        iou_scores = []\n",
        "\n",
        "        # Group detections and ground truth by frame\n",
        "        detections_by_frame = {}\n",
        "        for det in detections:\n",
        "            frame = det['frame']\n",
        "            if frame not in detections_by_frame:\n",
        "                detections_by_frame[frame] = []\n",
        "            detections_by_frame[frame].append(det)\n",
        "\n",
        "        # Evaluate each frame with ground truth\n",
        "        for frame_num, gt_boxes in ground_truth.items():\n",
        "            frame_detections = detections_by_frame.get(frame_num, [])\n",
        "            gt_matched = [False] * len(gt_boxes)\n",
        "            det_matched = [False] * len(frame_detections)\n",
        "\n",
        "            # Match detections with ground truth\n",
        "            for i, gt_box in enumerate(gt_boxes):\n",
        "                for j, det in enumerate(frame_detections):\n",
        "                    if not det_matched[j]:\n",
        "                        iou = self.calculate_iou(gt_box, det['bbox'])\n",
        "                        if iou >= iou_threshold:\n",
        "                            true_positives += 1\n",
        "                            gt_matched[i] = True\n",
        "                            det_matched[j] = True\n",
        "                            iou_scores.append(iou)\n",
        "                            break\n",
        "\n",
        "            # Count false positives and false negatives\n",
        "            false_positives += sum([1 for matched in det_matched if not matched])\n",
        "            false_negatives += sum([1 for matched in gt_matched if not matched])\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        mean_iou = np.mean(iou_scores) if iou_scores else 0\n",
        "\n",
        "        return {\n",
        "            'true_positives': true_positives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1_score,\n",
        "            'mean_iou': mean_iou,\n",
        "            'total_iou_scores': len(iou_scores)\n",
        "        }\n",
        "\n",
        "def create_sample_ground_truth(video_path, output_path, num_frames=3):\n",
        "    \"\"\"\n",
        "    Create sample ground truth data by manually annotating frames\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to input video\n",
        "        output_path: Path to save ground truth data\n",
        "        num_frames: Number of frames to annotate\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Select frames to annotate\n",
        "    frame_indices = [int(total_frames * (i+1) / (num_frames+1)) for i in range(num_frames)]\n",
        "\n",
        "    ground_truth = {}\n",
        "\n",
        "    for frame_idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            print(f\"Annotating frame {frame_idx}\")\n",
        "            cv2.imshow('Frame', frame)\n",
        "            cv2.waitKey(1)\n",
        "\n",
        "            # Simple manual annotation (in real scenario, use proper annotation tool)\n",
        "            print(\"Click and drag to draw bounding boxes. Press 'q' when done.\")\n",
        "\n",
        "            boxes = []\n",
        "            while True:\n",
        "                clone = frame.copy()\n",
        "                for box in boxes:\n",
        "                    cv2.rectangle(clone, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "\n",
        "                cv2.imshow('Annotation', clone)\n",
        "                key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "                if key == ord('q'):\n",
        "                    break\n",
        "                elif key == ord('r'):\n",
        "                    # Reset current frame annotations\n",
        "                    boxes = []\n",
        "                elif key == ord('a'):\n",
        "                    # Add bounding box (simulated - in real scenario, use mouse events)\n",
        "                    # For demo purposes, we'll add a sample box\n",
        "                    h, w = frame.shape[:2]\n",
        "                    sample_box = [w//4, h//4, 3*w//4, 3*h//4]\n",
        "                    boxes.append(sample_box)\n",
        "                    print(f\"Added box: {sample_box}\")\n",
        "\n",
        "            ground_truth[frame_idx] = boxes\n",
        "            cv2.destroyWindow('Annotation')\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # Save ground truth\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(ground_truth, f, indent=2)\n",
        "\n",
        "    print(f\"Ground truth saved to {output_path}\")\n",
        "    return ground_truth\n",
        "\n",
        "def generate_report(evaluation_results, output_path):\n",
        "    \"\"\"\n",
        "    Generate evaluation report\n",
        "\n",
        "    Args:\n",
        "        evaluation_results: Dictionary with evaluation metrics\n",
        "        output_path: Path to save the report\n",
        "    \"\"\"\n",
        "    report = f\"\"\"\n",
        "    OBJECT DETECTION EVALUATION REPORT\n",
        "    ==================================\n",
        "\n",
        "    Performance Metrics:\n",
        "    - True Positives: {evaluation_results['true_positives']}\n",
        "    - False Positives: {evaluation_results['false_positives']}\n",
        "    - False Negatives: {evaluation_results['false_negatives']}\n",
        "    - Precision: {evaluation_results['precision']:.3f}\n",
        "    - Recall: {evaluation_results['recall']:.3f}\n",
        "    - F1-Score: {evaluation_results['f1_score']:.3f}\n",
        "    - Mean IoU: {evaluation_results['mean_iou']:.3f}\n",
        "    - Total IoU Scores: {evaluation_results['total_iou_scores']}\n",
        "\n",
        "    Interpretation:\n",
        "    - Precision: Proportion of correct detections among all detections\n",
        "    - Recall: Proportion of ground truth objects that were detected\n",
        "    - F1-Score: Harmonic mean of precision and recall\n",
        "    - IoU: Measure of overlap between predicted and ground truth boxes\n",
        "    \"\"\"\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    print(f\"Report saved to {output_path}\")\n",
        "    return report\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the complete pipeline\n",
        "    \"\"\"\n",
        "    # Initialize detector\n",
        "    detector = VideoObjectDetector(model_name='yolov8n.pt', conf_threshold=0.5)\n",
        "\n",
        "    # File paths\n",
        "    input_video = \"/content/input_video.mp4\"\n",
        "    output_video = \"output_video.mp4\"\n",
        "    ground_truth_file = \"ground_truth.json\"\n",
        "    report_file = \"evaluation_report.txt\"\n",
        "\n",
        "    # Step 1: Create sample ground truth (if not exists)\n",
        "    if not Path(ground_truth_file).exists():\n",
        "        print(\"Creating sample ground truth...\")\n",
        "        ground_truth = create_sample_ground_truth(input_video, ground_truth_file)\n",
        "    else:\n",
        "        with open(ground_truth_file, 'r') as f:\n",
        "            ground_truth = json.load(f)\n",
        "\n",
        "    # Step 2: Process video and detect objects\n",
        "    print(\"Processing video for object detection...\")\n",
        "    results = detector.process_video(input_video, output_video, target_class='person')\n",
        "\n",
        "    # Step 3: Evaluate results\n",
        "    print(\"Evaluating detection results...\")\n",
        "    evaluation_results = detector.evaluate_detections(\n",
        "        results['detections'],\n",
        "        ground_truth\n",
        "    )\n",
        "\n",
        "    # Step 4: Generate report\n",
        "    print(\"Generating evaluation report...\")\n",
        "    report = generate_report(evaluation_results, report_file)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PROCESSING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Input video: {input_video}\")\n",
        "    print(f\"Output video: {output_video}\")\n",
        "    print(f\"Total frames processed: {results['video_info']['total_frames']}\")\n",
        "    print(f\"Total detections: {len(results['detections'])}\")\n",
        "    print(f\"Precision: {evaluation_results['precision']:.3f}\")\n",
        "    print(f\"Recall: {evaluation_results['recall']:.3f}\")\n",
        "    print(f\"F1-Score: {evaluation_results['f1_score']:.3f}\")\n",
        "    print(f\"Mean IoU: {evaluation_results['mean_iou']:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gfNTLW7cltN",
        "outputId": "945c6fd7-27fe-4ca4-def2-b51423ea8312"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing video for object detection...\n",
            "Processing video: /content/input_video.mp4\n",
            "Total frames: 1407\n",
            "Processed 30/1407 frames\n",
            "Processed 60/1407 frames\n",
            "Processed 90/1407 frames\n",
            "Processed 120/1407 frames\n",
            "Processed 150/1407 frames\n",
            "Processed 180/1407 frames\n",
            "Processed 210/1407 frames\n",
            "Processed 240/1407 frames\n",
            "Processed 270/1407 frames\n",
            "Processed 300/1407 frames\n",
            "Processed 330/1407 frames\n",
            "Processed 360/1407 frames\n",
            "Processed 390/1407 frames\n",
            "Processed 420/1407 frames\n",
            "Processed 450/1407 frames\n",
            "Processed 480/1407 frames\n",
            "Processed 510/1407 frames\n",
            "Processed 540/1407 frames\n",
            "Processed 570/1407 frames\n",
            "Processed 600/1407 frames\n",
            "Processed 630/1407 frames\n",
            "Processed 660/1407 frames\n",
            "Processed 690/1407 frames\n",
            "Processed 720/1407 frames\n",
            "Processed 750/1407 frames\n",
            "Processed 780/1407 frames\n",
            "Processed 810/1407 frames\n",
            "Processed 840/1407 frames\n",
            "Processed 870/1407 frames\n",
            "Processed 900/1407 frames\n",
            "Processed 930/1407 frames\n",
            "Processed 960/1407 frames\n",
            "Processed 990/1407 frames\n",
            "Processed 1020/1407 frames\n",
            "Processed 1050/1407 frames\n",
            "Processed 1080/1407 frames\n",
            "Processed 1110/1407 frames\n",
            "Processed 1140/1407 frames\n",
            "Processed 1170/1407 frames\n",
            "Processed 1200/1407 frames\n",
            "Processed 1230/1407 frames\n",
            "Processed 1260/1407 frames\n",
            "Processed 1290/1407 frames\n",
            "Processed 1320/1407 frames\n",
            "Processed 1350/1407 frames\n",
            "Processed 1380/1407 frames\n",
            "Evaluating detection results...\n",
            "Generating evaluation report...\n",
            "Report saved to evaluation_report.txt\n",
            "\n",
            "==================================================\n",
            "PROCESSING COMPLETE\n",
            "==================================================\n",
            "Input video: /content/input_video.mp4\n",
            "Output video: output_video.mp4\n",
            "Total frames processed: 1407\n",
            "Total detections: 2806\n",
            "Precision: 0.000\n",
            "Recall: 0.000\n",
            "F1-Score: 0.000\n",
            "Mean IoU: 0.000\n"
          ]
        }
      ]
    }
  ]
}